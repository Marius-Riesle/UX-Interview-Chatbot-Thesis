{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informativeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for informativeness calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Datei word_counts.pkl existiert bereits.\n"
     ]
    }
   ],
   "source": [
    "# Funktion zum Zählen von Wortstämmen\n",
    "def count_words(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "            text = file.read()\n",
    "            \n",
    "        # Tokenize the text into words\n",
    "        tokens = word_tokenize(text.lower())  # lowercase for uniformity\n",
    "\n",
    "        # Remove stopwords (common words that don't add much information)\n",
    "        stop_words = set(stopwords.words('german'))\n",
    "        tokens = [word for word in tokens if word not in stop_words and word.isalpha() and word != \"b\"]  # Keep only alphabetic words\n",
    "\n",
    "        # Perform stemming\n",
    "        stemmer = SnowballStemmer(\"german\")\n",
    "        stems = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "        # Häufigkeiten der Wortstämme zählen\n",
    "        word_counts = Counter(stems)\n",
    "\n",
    "        return word_counts\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Datei nicht gefunden: {file_path}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Ein Fehler ist aufgetreten: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Funktion zum Speichern als Pickle\n",
    "def save_as_pickle(data, output_file):\n",
    "    try:\n",
    "        with open(output_file, 'wb') as file:\n",
    "            pickle.dump(data, file)\n",
    "        print(f\"Dictionary gespeichert als: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Speichern der Datei: {e}\")\n",
    "\n",
    "# Pfad zur Textdatei\n",
    "file_path = 'DeReKo_text.txt'   \n",
    "pickle_file_path = 'word_counts.pkl'\n",
    "\n",
    "if os.path.exists(pickle_file_path):\n",
    "    print(f\"Die Datei {pickle_file_path} existiert bereits.\")\n",
    "else:\n",
    "    # Wortstämme zählen\n",
    "    word_counts = count_words(file_path)\n",
    "\n",
    "    # Normalisierte Dictionary als Pickle speichern\n",
    "    save_as_pickle(word_counts, pickle_file_path)\n",
    "\n",
    "    # Ausgabe\n",
    "    print(word_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informativeness function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file loaded successfully.\n",
      "Informativeness of user input: 5.219114520662927\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def load_word_frequencies():\n",
    "    # Load the pickle file\n",
    "    try:\n",
    "        with open(pickle_file_path, 'rb') as file:\n",
    "            word_frequencies = pickle.load(file)\n",
    "        print(\"Pickle file loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the pickle file: {e}\")\n",
    "    return word_frequencies\n",
    "\n",
    "# Preprocess the user input (tokenization, removing stopwords, stemming)\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text.lower())  # lowercase for uniformity\n",
    "\n",
    "    # Remove stopwords (common words that don't add much information)\n",
    "    stop_words = set(stopwords.words('german'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and word.isalpha()]  # Keep only alphabetic words\n",
    "\n",
    "    # Perform stemming\n",
    "    stemmer = SnowballStemmer(\"german\")\n",
    "    stems = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return stems\n",
    "\n",
    "# Calculate the surprisal of a word based on its frequency\n",
    "def calculate_surprisal(word, word_frequencies):\n",
    "    total_words = sum(word_frequencies.values())\n",
    "    word_freq = word_frequencies.get(word, 1)  # Default to 1 to avoid log(0)\n",
    "    probability = word_freq / total_words\n",
    "    return -math.log(probability)  \n",
    "\n",
    "# Calculate informativeness of a user's input (U)\n",
    "def calculate_informativeness(user_input, word_frequencies):\n",
    "    # Preprocess the user's input\n",
    "    words = preprocess_text(user_input)\n",
    "\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate the surprisal for each word\n",
    "    surprisals = [calculate_surprisal(word, word_frequencies) for word in words]\n",
    "\n",
    "    # Normalize the surprisals\n",
    "    min_surprisal = min(surprisals)\n",
    "    max_surprisal = max(surprisals)\n",
    "    \n",
    "    normalized_surprisals = [\n",
    "        (surprisal - min_surprisal) / (max_surprisal - min_surprisal) if max_surprisal != min_surprisal else 1\n",
    "        for surprisal in surprisals\n",
    "    ]   \n",
    "\n",
    "    # Calculate informativeness\n",
    "    informativeness = sum(normalized_surprisals)\n",
    "    \n",
    "    return informativeness\n",
    "\n",
    "# Example usage\n",
    "word_frequencies = load_word_frequencies()  \n",
    "user_input = \"Ich bin Manuel Neuer und liebe eckbälle und phillip lahm und wohne in Mannheim\"\n",
    "\n",
    "informativeness_score = calculate_informativeness(user_input, word_frequencies)\n",
    "\n",
    "print(f\"Informativeness of user input: {informativeness_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def calculate_metrics(dir:str, type:str, word_freqeuncies):\n",
    "    word_count = 0\n",
    "    answer_count = 0\n",
    "    informativeness = 0\n",
    "    if type == \"chatbot\":\n",
    "        for root, dirs, files in os.walk(dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\"Chat.txt\"):\n",
    "                    with open(os.path.join(dir, file), 'r', encoding='ISO-8859-1') as file:\n",
    "                        text = file.read()\n",
    "                        split_string = re.split(r'\\d{2}:\\d{2}:\\d{2} ', text)\n",
    "                        for absatz in split_string:\n",
    "                            if absatz.startswith(\"user:\"):\n",
    "                                if absatz[5:] != \"\":\n",
    "                                    answer_count += 1\n",
    "                                    word_count += len(absatz[5:].split())\n",
    "                                    informativeness += calculate_informativeness(absatz[5:], word_frequencies)\n",
    "\n",
    "    elif type == \"human\":\n",
    "        for root, dirs, files in os.walk(dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\"Transkript.txt\"):\n",
    "                    with open(os.path.join(dir, file), 'r', encoding='utf-8') as file:\n",
    "                        text = file.read()\n",
    "                        absatz_split = text.split(\"\\n\\n\")\n",
    "                        for absatz in absatz_split:\n",
    "                            if absatz.lstrip(\"\\n\").startswith(\"Proband\"):\n",
    "                                answer_text = \"\\n\".join(absatz.split(\"\\n\")[1:])\n",
    "                                if answer_text != \"\":\n",
    "                                    answer_count += 1\n",
    "                                    word_count += len(answer_text.split())\n",
    "                                    informativeness += calculate_informativeness(answer_text, word_frequencies)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"type has to be human or chatbot not: {type}\")\n",
    "\n",
    "    return (word_count, answer_count, informativeness)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file loaded successfully.\n",
      "Versuch_17\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics_for_all_dirs(base_directory:str,):\n",
    "    types = [\"human\", \"chatbot\"]\n",
    "    extracted_surveys = []\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d-%H-%M')  \n",
    "    word_frequencies = load_word_frequencies()\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for dir in dirs:\n",
    "            for type in types:\n",
    "                word_count_total, answer_count, informativeness = calculate_metrics(os.path.join(root, dir), type, word_frequencies)\n",
    "                # create placeholder for manual evaluation of other metrics\n",
    "                metrics = {\n",
    "                    \"Erkenntnisse\": {\n",
    "                        \"Nutzer\": [\"Aspekt 1\", \"Aspekt 2\"],\n",
    "                        \"Ziele\": [\"Aspekt 1\", \"Aspekt 2\"],\n",
    "                        \"Aufgaben\": [\"Aspekt 1\", \"Aspekt 2\"],\n",
    "                        \"Umgebung\": [\"Aspekt 1\", \"Aspekt 2\"],\n",
    "                        \"Ressourcen\": [\"Aspekt 1\", \"Aspekt 2\"]\n",
    "                    },\n",
    "                    \"Woerter_gesamt\": word_count_total,\n",
    "                    \"informativität\": informativeness,\n",
    "                    \"fragen_gesamt\": answer_count,\n",
    "                    \"klarstellungsanfragen\": -1,\n",
    "                    \"leading_biased_fragen\": -1,\n",
    "                    \"geschlossene_fragen\": -1,\n",
    "                    \"interview_laenge\": -1,\n",
    "                }\n",
    "\n",
    "                \n",
    "\n",
    "                json_file_name = f\"metrics_{type}_{current_date}.json\"\n",
    "                json_file_path = os.path.join(root, dir, json_file_name)\n",
    "                with open(json_file_path, 'w') as json_file:\n",
    "                    json.dump(metrics, json_file, indent=4)\n",
    "            \n",
    "\n",
    "    return extracted_surveys\n",
    "\n",
    "base_directory = \"..\\..\\Interviewdaten\"\n",
    "extracted_human_surveys = calculate_metrics_for_all_dirs(base_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
